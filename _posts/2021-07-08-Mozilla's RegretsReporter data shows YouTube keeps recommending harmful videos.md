<img src='https://cdn.vox-cdn.com/thumbor/6ug_eZYu8WtG55S202TdqUjHV2g=/0x0:2040x1360/1200x800/filters:focal(857x517:1183x843)/cdn.vox-cdn.com/uploads/chorus_image/image/69554546/acastro_180321_1777_youtube_0002.0.jpg' width='700px' /><br/>
That the machine learning-driven feed of YouTube recommendations can frequently surface results of an edgy or even radicalizing bent isn't much of a question anymore. YouTube itself has pushed tools that it says could give users more control over their feed and transparency about certain recommendations, but it's difficult for outsiders to know what kind of impact they're having. Now, after spending much of the last year collecting data from the RegretsReporter extension (available for Firefox or Chrome), the Mozilla Foundation has more information on what people see when the algorithm makes the wrong choice and has released a detailed report (pdf).
<a href='https://www.theverge.com/2021/7/7/22567640/youtube-algorithm-suggestions-radicalization-mozilla'> Source <a/>